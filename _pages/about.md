---
permalink: /
title: "Ziyao Wang (王子瑶)"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<div class="page__hero--buttons" style="margin: 0.6rem 0 1.2rem;">
  <a class="btn btn--primary" href="/cv/">CV</a>
  <a class="btn btn--info" href="/publications/">Publications</a>
  <a class="btn btn--success" href="https://scholar.google.com/citations?user=YOUR_ID" target="_blank" rel="noopener">Google Scholar</a>
  <a class="btn btn--inverse" href="https://github.com/YOUR_GITHUB" target="_blank" rel="noopener">GitHub</a>
</div>

I’m a **third-year Ph.D. student** in ECE at **University of Maryland, College Park**, advised by Prof. [Ang Li](https://www.ang-li.com/). Previously, I received my B.E. in Computer Engineering from **Wuhan University**, where I worked with Prof. [Qian Wang](http://nisplab.whu.edu.cn/). I was a research intern at **IBM Research (Summer 2024)** and **Sony AI (Summer 2025)**, focusing on **collaborative and efficient foundation models**.

My research aims to make foundation models **more collaborative, more efficient, and more reliable**—especially in **federated/edge settings**.

---

## Research Focus
- **Federated & collaborative foundation models:** federated fine-tuning, heterogeneous personalization, communication-efficient learning  
- **Efficient LLM systems:** low-rank adaptation, decoding-time collaboration, practical deployment constraints  
- **Reliable AI services:** auditing/measurement of opaque LLM behaviors, robustness and safety (system-level)

---

## News
- *(2025)* Paper accepted at **ICML 2025**.
- *(2025)* Internship at **Sony AI**.
- *(2024)* Paper accepted at **NeurIPS 2024**.

---

## Selected Publications
(* = equal contribution)

<div class="selected-pubs">

<p>
<strong>Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding</strong><br>
<strong>Ziyao Wang</strong>, Muneeza Azmat, Ang Li, Raya Horesh, Mikhail Yurochkin.<br>
<strong>ICML 2025</strong>
</p>

<p>
<strong>FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations</strong><br>
<strong>Ziyao Wang</strong>, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, Ang Li.<br>
<strong>NeurIPS 2024</strong>
</p>

<p>
<strong>FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent</strong><br>
<strong>Ziyao Wang</strong>, Jianyu Wang, Ang Li.<br>
<strong>ICLR 2024</strong>
</p>

<p>
<strong>UPTON: Unattributable Authorship Text via Data Poisoning</strong><br>
<strong>Ziyao Wang</strong>, Dongwon Lee, Thai Le.<br>
<strong>EMNLP 2023</strong>
</p>

<p>
<strong>Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices</strong><br>
<strong>Ziyao Wang</strong>, Yexiao He, Zheyu Shen, Yu Li, Guoheng Sun, Myungjin Lee, Ang Li.<br>
<a href="https://arxiv.org/abs/2503.14932" target="_blank" rel="noopener">arXiv:2503.14932</a>
</p>

<p>
<strong>One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models</strong><br>
<strong>Ziyao Wang</strong>, Bowei Tian, Yexiao He, Zheyu Shen, Luyang Liu, Ang Li.<br>
<a href="https://arxiv.org/abs/2412.04650" target="_blank" rel="noopener">arXiv:2412.04650</a>
</p>

<p>
<strong>Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation</strong><br>
<strong>Ziyao Wang*</strong>, Guoheng Sun*, Yexiao He, Zheyu Shen, Bowei Tian, Ang Li.<br>
<a href="https://arxiv.org/abs/2508.00912" target="_blank" rel="noopener">arXiv:2508.00912</a>
</p>

<p>
<strong>Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services</strong><br>
Guoheng Sun*, <strong>Ziyao Wang*</strong>, Xuandong Zhao, Bowei Tian, Zheyu Shen, Yexiao He, Jinming Xing, Ang Li.<br>
<a href="https://arxiv.org/abs/2505.18471" target="_blank" rel="noopener">arXiv:2505.18471</a>
</p>

<p>
<strong>A GAN-based Defense Framework against Model Inversion Attacks</strong><br>
Xueluan Gong*, <strong>Ziyao Wang*</strong>, Yanjiao Chen, Qian Wang.<br>
<strong>TIFS</strong>
</p>

<p>
<strong>Kaleidoscope: Triggering Backdoor with RGB Filters</strong><br>
Xueluan Gong*, <strong>Ziyao Wang*</strong>, Yuzhe Gu, Yanjiao Chen, Qian Wang.<br>
<strong>TDSC</strong>
</p>

</div>

**Full list:** see [/publications/](/publications/)  
<span style="opacity:0.75;">Last updated: {{ site.time | date: "%Y-%m-%d" }}</span>
